from __future__ import annotations

import re
import string
import unicodedata
from itertools import chain
from typing import Dict, Iterator, List, Literal, Optional, Set, cast
from urllib.parse import ParseResult, urlparse, urlunparse

from osprey.engine.stdlib.udfs._prelude import (
    ArgumentsBase,
    ExecutionContext,
    UDFBase,
    ValidationContext,
)
from unidecode import unidecode

from .categories import UdfCategories


class StringArguments(ArgumentsBase):
    s: str


class StringLength(UDFBase[StringArguments, int]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringArguments) -> int:
        return len(arguments.s)


class StringToLower(UDFBase[StringArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringArguments) -> str:
        return arguments.s.lower()


class StringToUpper(UDFBase[StringArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringArguments) -> str:
        return arguments.s.upper()


class StringStartsWithArgument(StringArguments):
    s: str
    start: str


class StringStartsWith(UDFBase[StringStartsWithArgument, bool]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringStartsWithArgument) -> bool:
        return arguments.s.startswith(arguments.start)


class StringEndsWithArgument(StringArguments):
    end: str


class StringEndsWith(UDFBase[StringEndsWithArgument, bool]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringEndsWithArgument) -> bool:
        return arguments.s.endswith(arguments.end)


class StringStripArguments(StringArguments):
    chars: Optional[str] = None


class StringStrip(UDFBase[StringStripArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringStripArguments) -> str:
        return arguments.s.strip(arguments.chars)


class StringRStrip(UDFBase[StringStripArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringStripArguments) -> str:
        return arguments.s.rstrip(arguments.chars)


class StringLStrip(UDFBase[StringStripArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringStripArguments) -> str:
        return arguments.s.lstrip(arguments.chars)


class StringReplaceArguments(StringArguments):
    old: str
    new: str


class StringReplace(UDFBase[StringReplaceArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringReplaceArguments) -> str:
        return arguments.s.replace(arguments.old, arguments.new)


class StringJoinArguments(StringArguments):
    iterable: List[str]


class StringJoin(UDFBase[StringJoinArguments, str]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringJoinArguments) -> str:
        return arguments.s.join(arguments.iterable)


class StringSplitArguments(StringArguments):
    sep: Optional[str] = None
    maxsplit: int = -1


class StringSplit(UDFBase[StringSplitArguments, List[str]]):
    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringSplitArguments) -> List[str]:
        return arguments.s.split(arguments.sep, arguments.maxsplit)


class StringCleaningArguments(StringArguments):
    form: str = 'NFKC'

    # normalizations happen in this order, so lower will win over upper
    remove_emoji: bool = False

    # reduce any repeated space to a single to a single normal space (U+0020)
    space: bool = True

    # remove any l33t code, these are homoglyphs that are less direct and may have contextual meaning
    l33t: bool = False

    # remove clear homoglyphs that are stylization of letters
    homoglyph: bool = True

    # deconstruct any combined unicode and keep the first char ('Ã‡' -> 'C')
    unicode_normalize: bool = True

    # replace any unicode with english transliteration (bad for l33t), good for roman-ish script 'ÎšÎ½Ï‰ÏƒÏŒÏ‚' -> 'Knosos'
    unidecode: bool = False

    # uppercase the string (lower takes precedence over upper)
    upper: bool = False

    # lower case the string
    lower: bool = False

    # remove all spaces from the string
    remove_space: bool = False

    # remove all punctuation from the string (using unicodedata.category SP)
    remove_punctuation: bool = False


TranslationT = Dict[int, Optional[int]]


_SPACE_PATTERN: re.Pattern[str] = re.compile(r'\s+')

_EMOJI_PATTERN: re.Pattern[str] = re.compile(
    r'['
    r'\U0001F600-\U0001F64F'  # emoticons
    r'\U0001F300-\U0001F5FF'  # symbols & pictographs
    r'\U0001F680-\U0001F6FF'  # transport & map symbols
    r'\U0001F1E0-\U0001F1FF'  # flags (iOS)
    r']+',
    flags=re.UNICODE,
)

# sub for l33t -> leet
_L33T_THREES_SUB_PATTERN: re.Pattern[str] = re.compile(r'([A-z]?)(3+)([A-z]?)', flags=re.IGNORECASE)

# sub for |7 -> 17
_L33T_PIPE_NUMBER_SUB_PATTERN: re.Pattern[str] = re.compile(r'\|(\d)')

# homoglyphs for ascii letters from the homoglyphs lib
_HOMOGLYPHS = {
    'a': 'âºğ€ğšğ´ğ‘ğ‘¨ğ’‚ğ’œğ’¶ğ“ğ“ªğ”„ğ”ğ”¸ğ•’ğ•¬ğ–†ğ– ğ–ºğ—”ğ—®ğ˜ˆğ˜¢ğ˜¼ğ™–ğ™°ğšŠğš¨ğ›‚ğ›¢ğ›¼ğœœğœ¶ğ–ğ°ğğª',
    'b': 'â„¬ğğ›ğµğ‘ğ‘©ğ’ƒğ’·ğ“‘ğ“«ğ”…ğ”Ÿğ”¹ğ•“ğ•­ğ–‡ğ–¡ğ–»ğ—•ğ—¯ğ˜‰ğ˜£ğ˜½ğ™—ğ™±ğš‹ğš©ğ›£ğœğ—ğ‘',
    'c': 'â„‚â„­ğ‚ğœğ¶ğ‘ğ‘ªğ’„ğ’ğ’¸ğ“’ğ“¬ğ” ğ•”ğ•®ğ–ˆğ–¢ğ–¼ğ—–ğ—°ğ˜Šğ˜¤ğ˜¾ğ™˜ğ™²ğšŒğŸŒ',
    'd': 'â……â…†ğƒğğ·ğ‘‘ğ‘«ğ’…ğ’Ÿğ’¹ğ““ğ“­ğ”‡ğ”¡ğ”»ğ••ğ•¯ğ–‰ğ–£ğ–½ğ——ğ—±ğ˜‹ğ˜¥ğ˜¿ğ™™ğ™³ğš',
    'e': 'â„®â„¯â„°â…‡â‹¿ğ„ğğ¸ğ‘’ğ‘¬ğ’†ğ“”ğ“®ğ”ˆğ”¢ğ”¼ğ•–ğ•°ğ–Šğ–¤ğ–¾ğ—˜ğ—²ğ˜Œğ˜¦ğ™€ğ™šğ™´ğšğš¬ğ›¦ğœ ğšğ”',
    'f': 'â„±ğ…ğŸğ¹ğ‘“ğ‘­ğ’‡ğ’»ğ“•ğ“¯ğ”‰ğ”£ğ”½ğ•—ğ•±ğ–‹ğ–¥ğ–¿ğ—™ğ—³ğ˜ğ˜§ğ™ğ™›ğ™µğšğŸŠ',
    'g': 'â„Šğ†ğ ğºğ‘”ğ‘®ğ’ˆğ’¢ğ“–ğ“°ğ”Šğ”¤ğ”¾ğ•˜ğ•²ğ–Œğ–¦ğ—€ğ—šğ—´ğ˜ğ˜¨ğ™‚ğ™œğ™¶ğš',
    'h': 'â„‹â„Œâ„â„ğ‡ğ¡ğ»ğ‘¯ğ’‰ğ’½ğ“—ğ“±ğ”¥ğ•™ğ•³ğ–ğ–§ğ—ğ—›ğ—µğ˜ğ˜©ğ™ƒğ™ğ™·ğš‘ğš®ğ›¨ğœ¢ğœğ–',
    'i': 'lË›â„¹â…ˆâ³ğ¢ğ‘–ğ’Šğ’¾ğ“²ğ”¦ğ•šğ–ğ—‚ğ—¶ğ˜ªğ™ğš’ğš¤ğ›Šğœ„ğœ¾ğ¸ğ²',
    'j': 'â…‰ğ‰ğ£ğ½ğ‘—ğ‘±ğ’‹ğ’¥ğ’¿ğ“™ğ“³ğ”ğ”§ğ•ğ•›ğ•µğ–ğ–©ğ—ƒğ—ğ—·ğ˜‘ğ˜«ğ™…ğ™Ÿğ™¹ğš“',
    'k': 'ğŠğ¤ğ¾ğ‘˜ğ‘²ğ’Œğ’¦ğ“€ğ“šğ“´ğ”ğ”¨ğ•‚ğ•œğ•¶ğ–ğ–ªğ—„ğ—ğ—¸ğ˜’ğ˜¬ğ™†ğ™ ğ™ºğš”ğš±ğ›«ğœ¥ğŸğ™',
    'l': '1I|â„â„‘â„’â„“âˆ£â½ï¿¨ğˆğ‹ğ¥ğ¼ğ¿ğ‘™ğ‘°ğ‘³ğ’ğ“ğ“˜ğ“›ğ“µğ”ğ”©ğ•€ğ•ƒğ•ğ•´ğ•·ğ–‘ğ–¨ğ–«ğ—…ğ—œğ—Ÿğ—¹ğ˜ğ˜“ğ˜­ğ™„ğ™‡ğ™¡ğ™¸ğ™»ğš•ğš°ğ›ªğœ¤ğğ˜ğŸğŸ™ğŸ£ğŸ­ğŸ·',
    'm': 'â„³ğŒğ‘€ğ‘´ğ“œğ”ğ•„ğ•¸ğ–¬ğ— ğ˜”ğ™ˆğ™¼ğš³ğ›­ğœ§ğ¡ğ›',
    'n': 'â„•ğğ§ğ‘ğ‘›ğ‘µğ’ğ’©ğ“ƒğ“ğ“·ğ”‘ğ”«ğ•Ÿğ•¹ğ–“ğ–­ğ—‡ğ—¡ğ—»ğ˜•ğ˜¯ğ™‰ğ™£ğ™½ğš—ğš´ğ›®ğœ¨ğ¢ğœ',
    'o': '0â„´ğğ¨ğ‘‚ğ‘œğ‘¶ğ’ğ’ªğ“ğ“¸ğ”’ğ”¬ğ•†ğ• ğ•ºğ–”ğ–®ğ—ˆğ—¢ğ—¼ğ˜–ğ˜°ğ™Šğ™¤ğ™¾ğš˜ğš¶ğ›ğ›”ğ›°ğœŠğœğœªğ„ğˆğ¤ğ¾ğ‚ğğ¸ğ¼ğŸğŸ˜ğŸ¢ğŸ¬ğŸ¶',
    'p': 'â„™â´ğğ©ğ‘ƒğ‘ğ‘·ğ’‘ğ’«ğ“…ğ“Ÿğ“¹ğ”“ğ”­ğ•¡ğ•»ğ–•ğ–¯ğ—‰ğ—£ğ—½ğ˜—ğ˜±ğ™‹ğ™¥ğ™¿ğš™ğš¸ğ›’ğ› ğ›²ğœŒğœšğœ¬ğ†ğ”ğ¦ğ€ğğ ğºğŸˆ',
    'q': 'â„šğğªğ‘„ğ‘ğ‘¸ğ’’ğ’¬ğ“†ğ“ ğ“ºğ””ğ”®ğ•¢ğ•¼ğ––ğ–°ğ—Šğ—¤ğ—¾ğ˜˜ğ˜²ğ™Œğ™¦ğš€ğšš',
    'r': 'â„›â„œâ„ğ‘ğ«ğ‘…ğ‘Ÿğ‘¹ğ’“ğ“‡ğ“¡ğ“»ğ”¯ğ•£ğ•½ğ–—ğ–±ğ—‹ğ—¥ğ—¿ğ˜™ğ˜³ğ™ğ™§ğšğš›',
    's': 'ğ’ğ¬ğ‘†ğ‘ ğ‘ºğ’”ğ’®ğ“ˆğ“¢ğ“¼ğ”–ğ”°ğ•Šğ•¤ğ•¾ğ–˜ğ–²ğ—Œğ—¦ğ˜€ğ˜šğ˜´ğ™ğ™¨ğš‚ğšœ',
    't': 'âŠ¤âŸ™ğ“ğ­ğ‘‡ğ‘¡ğ‘»ğ’•ğ’¯ğ“‰ğ“£ğ“½ğ”—ğ”±ğ•‹ğ•¥ğ•¿ğ–™ğ–³ğ—ğ—§ğ˜ğ˜›ğ˜µğ™ğ™©ğšƒğšğš»ğ›µğœ¯ğ©ğ£ğŸ¨',
    'u': 'âˆªâ‹ƒğ”ğ®ğ‘ˆğ‘¢ğ‘¼ğ’–ğ’°ğ“Šğ“¤ğ“¾ğ”˜ğ”²ğ•Œğ•¦ğ–€ğ–šğ–´ğ—ğ—¨ğ˜‚ğ˜œğ˜¶ğ™ğ™ªğš„ğšğ›–ğœğŠğ„ğ¾',
    'v': 'âˆ¨â‹ğ•ğ¯ğ‘‰ğ‘£ğ‘½ğ’—ğ’±ğ“‹ğ“¥ğ“¿ğ”™ğ”³ğ•ğ•§ğ–ğ–›ğ–µğ—ğ—©ğ˜ƒğ˜ğ˜·ğ™‘ğ™«ğš…ğšŸğ›ğœˆğ‚ğ¼ğ¶',
    'w': 'ğ–ğ°ğ‘Šğ‘¤ğ‘¾ğ’˜ğ’²ğ“Œğ“¦ğ”€ğ”šğ”´ğ•ğ•¨ğ–‚ğ–œğ–¶ğ—ğ—ªğ˜„ğ˜ğ˜¸ğ™’ğ™¬ğš†ğš ',
    'x': 'Ã—â•³â¤«â¤¬â¨¯ğ—ğ±ğ‘‹ğ‘¥ğ‘¿ğ’™ğ’³ğ“ğ“§ğ”ğ”›ğ”µğ•ğ•©ğ–ƒğ–ğ–·ğ—‘ğ—«ğ˜…ğ˜Ÿğ˜¹ğ™“ğ™­ğš‡ğš¡ğš¾ğ›¸ğœ²ğ¬ğ¦',
    'y': 'â„½ğ˜ğ²ğ‘Œğ‘¦ğ’€ğ’šğ’´ğ“ğ“¨ğ”‚ğ”œğ”¶ğ•ğ•ªğ–„ğ–ğ–¸ğ—’ğ—¬ğ˜†ğ˜ ğ˜ºğ™”ğ™®ğšˆğš¢ğš¼ğ›„ğ›¶ğ›¾ğœ°ğœ¸ğªğ²ğ¤ğ¬',
    'z': 'â„¤â„¨ğ‹µğ™ğ³ğ‘ğ‘§ğ’ğ’›ğ’µğ“ğ“©ğ”ƒğ”·ğ•«ğ–…ğ–Ÿğ–¹ğ—“ğ—­ğ˜‡ğ˜¡ğ˜»ğ™•ğ™¯ğš‰ğš£ğš­ğ›§ğœ¡ğ›ğ•',
}

# extra homoglyphs that we have found useful
_HOMOGLYPHS_EXTRA = {
    'a': '@ÂªÎ±âˆ€âŸ‘',
    'b': 'Î²Ğ’ÑŒà¸¿',
    'c': 'Â¢Â©Ã‡Ã§âˆâŠ‚Ï²',
    'd': 'áƒ«âˆ‚â«’',
    'e': 'Ï±â‚¬â„‡â„®âˆƒâˆˆâˆ‘â‹¿',
    'f': 'ÏáŸ›â¨â¨—â«­ğ…¿',
    'g': 'ÔŒÖâ‚²',
    'h': 'â‚¶â„â«²â«³',
    'i': 'Î¹Ñ—â«¯',
    'j': 'Ï³Ñ˜âŒ¡',
    'k': 'ÎºÏâ‚­',
    'l': '|Õ¬â‚¤âˆŸ',
    'm': 'â‚¥â‰â‹”â¨‡â©‹â«™',
    'n': 'Î Î·Ï€âˆâˆ©åˆ€',
    'o': 'ÂºÎ¿Ö…â˜‰â¦¿',
    'p': 'Î¡ÏÕ¢â‚±â„—â™‡',
    'q': 'Ò©Ô›Õ£Õ¦à§­',
    'r': 'Â®Ğ¯Õ’ğ…¾',
    's': '$Ñ•âˆ«',
    't': 'Ï„Õ§âŠºâ™°â™±âŸ™',
    'u': 'Âµâˆâˆªâ¨ƒ',
    'v': 'âˆšâˆ¨â©”',
    'w': 'Ï‰â‚©â¨ˆâ©Šâ«',
    'x': 'Ã—â˜“âœ—â¨¯',
    'y': 'Â¥Ó±áƒ„â‘‚',
    'z': 'Õ€áƒ€ê™€',
}

_HOMOGLYPHS_ENCLOSED = {
    a: {chr(ord(enclosure) + (ord(a) - ord('a'))) for enclosure in ['â’œ', 'â’¶', 'ğŸ„', 'ğŸ„°', 'ğŸ…', 'ğŸ…°', 'ğŸ‡¦']}
    for a in string.ascii_lowercase
}

# translate emoji range lookalikes to ascii
_HOMOGLYPHS_EMOJI_TRANSLATION_TABLE: TranslationT = str.maketrans(
    {  # type: ignore[arg-type]
        glyph: ord(alpha)
        for alpha, glyphs in chain(
            _HOMOGLYPHS.items(),
            _HOMOGLYPHS_EXTRA.items(),
            _HOMOGLYPHS_ENCLOSED.items(),
        )
        for glyph in glyphs
        if _EMOJI_PATTERN.match(glyph)
    }
)

# translate unicode lookalikes to ascii
_HOMOGLYPHS_TRANSLATION_TABLE: TranslationT = str.maketrans(
    {  # type: ignore[arg-type]
        glyph: ord(alpha)
        for alpha, glyphs in chain(
            _HOMOGLYPHS.items(),
            _HOMOGLYPHS_EXTRA.items(),
            _HOMOGLYPHS_ENCLOSED.items(),
        )
        for glyph in glyphs
        if not (
            # digits that get in here somehow should be handled by 'l33t'
            glyph.isdigit()
            # a regular letter isn't a homoglyph for this, handle that in 'l33t'
            # e.g. 'I' != 'l' for homoglyphs, but might in the right context
            or glyph in string.ascii_letters
        )
    }
)


class StringClean(UDFBase[StringCleaningArguments, str]):
    """
    String cleaning swiss army knife
    """

    category = UdfCategories.STRING

    def __init__(self, validation_context: ValidationContext, arguments: StringCleaningArguments):
        super().__init__(validation_context, arguments)
        if arguments.form not in ['NFC', 'NFKC', 'NFD', 'NFKD']:
            call_node = arguments.get_call_node()
            validation_context.add_error(
                message='invalid value for `form`',
                span=call_node.span,
                hint=(f'`form` must be one of `NFC`, `NFKC`, `NFD`, or `NFKD`, not `{arguments.form}`'),
            )

    @staticmethod
    def _sub_l33t_3_to_e_helper(m: re.Match[str]) -> str:
        # TODO: there is probably a much better solution for this
        return f'{m[1]}{"e" * len(m[2])}{m[3]}' if m[1] or m[3] else m[2]

    def execute(self, execution_context: ExecutionContext, arguments: StringCleaningArguments) -> str:
        s = arguments.s

        if arguments.remove_emoji:
            if arguments.homoglyph:
                # the intent is probably not to remove these
                s = s.translate(_HOMOGLYPHS_EMOJI_TRANSLATION_TABLE)
            s = _EMOJI_PATTERN.sub(r' ', s)

        if arguments.space:
            s = _SPACE_PATTERN.sub(r' ', s)

        if arguments.l33t:
            s = _L33T_PIPE_NUMBER_SUB_PATTERN.sub(r'1\1', s)
            s = _L33T_THREES_SUB_PATTERN.sub(self._sub_l33t_3_to_e_helper, s)

        if arguments.homoglyph:
            s = s.replace('â„¹ï¸', 'i')  # â„¹ï¸ is multi byte and is incompatible with str.translate
            s = s.translate(_HOMOGLYPHS_TRANSLATION_TABLE)  # needs to go after l33t regex work

        if arguments.unicode_normalize:
            # We know that arguments.form has type Literal[...] because of the validation in __init__.
            # Ideally we could type this in StringCleaningArguments but Osprey's type evaluator
            # doesn't support Literals so we keep it as str and cast it here
            arguments.form = cast(Literal['NFC', 'NFKC', 'NFD', 'NFKD'], arguments.form)
            new_s = unicodedata.normalize(arguments.form, s)

            if len(s) != len(new_s):
                # the new string had multi-byte chars in it, remove them individually
                new_s = ''.join(unicodedata.normalize(arguments.form, _)[0] for _ in s)

            s = new_s

        if arguments.unidecode:
            s = unidecode(s)

        if arguments.upper and not arguments.lower:
            s = s.upper()

        if arguments.lower:
            s = s.lower()

        if arguments.remove_space:
            s = _SPACE_PATTERN.sub(r'', s)

        if arguments.remove_punctuation:
            s = ''.join(ch for ch in s if unicodedata.category(ch)[0] not in 'SP')

        return s


class StringExtractDomains(UDFBase[StringArguments, List[str]]):
    """
    Used to extract a list of potential URL domains from a string of tokens. Returns a list
    of candidate domains encountered in the input string. Should be used in conjunction with
    other UDFs that expect a domain as an input
    """

    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringArguments) -> List[str]:
        # split the message into individual tokens as based on a modified URL regex from messages_common.
        # should capture space based links and markdown based links without duplication.
        potential_urls: Iterator[ParseResult] = (
            urlparse(token) for token in re.findall('(https?:\/\/[^\/\s][^\s\)>]+)', arguments.s)
        )

        # filter out any tokens that do not have a scheme or a domain
        valid_domains: Set[str] = set(url.netloc.split(':')[0] for url in potential_urls if url.scheme and url.netloc)

        # return any valid domains encountered in the message
        return list(valid_domains)


class StringExtractURLs(UDFBase[StringArguments, List[str]]):
    """
    Used to extract a list of potential URLs from a string of tokens. Returns a list
    of candidate URLs encountered in the input string. Should be used in conjunction with
    other UDFs that expect an url as an input
    """

    category = UdfCategories.STRING

    def execute(self, execution_context: ExecutionContext, arguments: StringArguments) -> List[str]:
        # split the message into individual tokens as based on a modified URL regex from messages_common.
        # should capture space based links and markdown based links without duplication.
        potential_urls: Iterator[ParseResult] = (
            urlparse(token) for token in re.findall('(https?:\/\/[^\/\s][^\s\)>]+)', arguments.s)
        )

        # filter out any tokens that do not have a scheme or a domain
        valid_urls: Set[str] = set(
            urlunparse(parsed_url) for parsed_url in potential_urls if parsed_url.scheme and parsed_url.netloc
        )

        # return any valid urls encountered in the message
        return list(valid_urls)
